# -*- coding: utf-8 -*-
"""Untitled58.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m8oYwnUZlSg4adpgBPaPhPW2cZPGnBJX
"""

from google.colab import files
import zipfile
uploaded = files.upload()  # select your Stroke_classification.zip

# Unzip
for filename in uploaded.keys():
    zip_ref = zipfile.ZipFile(filename, 'r')
    zip_ref.extractall("/content/")
    zip_ref.close()

dataset_path = "/content/Stroke_classification"

!pip install -q tensorflow   # or latest stable TF in your runtime

import os, numpy as np, matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess
from sklearn.metrics import classification_report, confusion_matrix
import itertools

IMG_SIZE = (224, 224)
BATCH_SIZE = 16
SEED = 42

# Generators
train_datagen = ImageDataGenerator(
    preprocessing_function=eff_preprocess,
    rotation_range=15,
    width_shift_range=0.08,
    height_shift_range=0.08,
    zoom_range=0.08,
    horizontal_flip=True,
    vertical_flip=False,
    fill_mode='nearest',
    validation_split=0.2 # 20% validation split
)

valid_datagen = ImageDataGenerator(preprocessing_function=eff_preprocess, validation_split=0.2)

train_generator = train_datagen.flow_from_directory(
    dataset_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True,
    seed=SEED
)

valid_generator = valid_datagen.flow_from_directory(
    dataset_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False,
    seed=SEED
)

# Inspect class indices and counts
print("Class indices:", train_generator.class_indices)

from collections import Counter

# Count from the training generator samples per class
train_counts = Counter()
for cls, idx in train_generator.class_indices.items():
    train_counts[cls] = len(os.listdir(os.path.join(dataset_path, cls))) # full folder counts

# Alternatively use train_generator.classes for exact number in subset:
# unique, counts = np.unique(train_generator.classes, return_counts=True)
# map them to class names with train_generator.class_indices.

print("Folder counts (approx):", train_counts)

# If using exact counts from generator subset:
unique, counts = np.unique(train_generator.classes, return_counts=True)
class_counts = dict(zip(unique, counts))
print("Training subset counts:", class_counts)

# Compute sklearn-style class weight dict
from sklearn.utils.class_weight import compute_class_weight
classes = np.unique(train_generator.classes)
class_weights = compute_class_weight('balanced', classes=classes, y=train_generator.classes)
class_weight_dict = dict(zip(classes, class_weights))
print("Class weights:", class_weight_dict)

def build_model(input_shape=(224,224,3), n_classes=3, dropout_rate=0.4):
    base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)
    base.trainable = False  # freeze initially

    x = layers.GlobalAveragePooling2D()(base.output)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(dropout_rate)(x)
    outputs = layers.Dense(n_classes, activation='softmax')(x)

    model = models.Model(inputs=base.input, outputs=outputs)
    return model

model = build_model(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), n_classes=3)
model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

checkpoint_path = '/content/drive/MyDrive/stroke_best_model.h5'

cb = [
    callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1),
    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1),
    callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)
]

EPOCHS = 30

history = model.fit(
    train_generator,
    validation_data=valid_generator,
    epochs=EPOCHS,
    class_weight=class_weight_dict,
    callbacks=cb
)

# Load best model
model.load_weights(checkpoint_path)

# Predict on validation set (or a separate held-out test set)
valid_generator.reset()
preds = model.predict(valid_generator, verbose=1)
y_true = valid_generator.classes
y_pred = np.argmax(preds, axis=1)
class_names = list(valid_generator.class_indices.keys())

# Confusion matrix & classification report
print(classification_report(y_true, y_pred, target_names=class_names))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6,6))
plt.imshow(cm, interpolation='nearest')
plt.title('Confusion matrix')
plt.colorbar()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names, rotation=45)
plt.yticks(tick_marks, class_names)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

import tensorflow.keras.backend as K
import cv2

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # img_array: preprocessed image, shape (1, H, W, 3)
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)
    return heatmap.numpy()

def overlay_heatmap(img_path, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):
    # Read original image
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, colormap)
    overlay = cv2.addWeighted(img, 1-alpha, heatmap, alpha, 0)
    return overlay

# Example usage:
# 1) get last conv layer name
for layer in reversed(model.layers):
    if isinstance(layer, layers.Conv2D) or 'conv' in layer.name:
        last_conv_layer_name = layer.name
        break
print("Last conv layer:", last_conv_layer_name)

# 2) pick an image from validation set or path
img_path = '/content/Ellappan T2-11.jpg_Ischemic_9.png'  # example
img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)
img_array = tf.keras.preprocessing.image.img_to_array(img)
input_arr = eff_preprocess(np.expand_dims(img_array.copy(), axis=0))  # preprocess

preds = model.predict(input_arr)
pred_class = np.argmax(preds[0])
print("Predicted:", class_names[pred_class], "Confidence:", preds[0][pred_class])

heatmap = make_gradcam_heatmap(input_arr, model, last_conv_layer_name, pred_index=pred_class)
overlay_img = overlay_heatmap(img_path, heatmap)

plt.figure(figsize=(10,5))
plt.subplot(1,2,1); plt.title("Original"); plt.axis('off'); plt.imshow(img)
plt.subplot(1,2,2); plt.title("Grad-CAM Overlay"); plt.axis('off'); plt.imshow(overlay_img)
plt.show()

# Save full model (architecture + weights) to Google Drive in HDF5 format
# Make sure you have mounted your Google Drive first
# from google.colab import drive
# drive.mount('/content/drive')
model.save('/content/drive/MyDrive/stroke_model_full.h5')

# OR save TF SavedModel format for deployment using model.export()
model.export('/content/drive/MyDrive/stroke_saved_model')

# Option to save to the local Colab environment for direct download in HDF5 format
model.save('/content/stroke_model_local.h5')
# You can then download 'stroke_model_local.h5' from the files pane in Colab



